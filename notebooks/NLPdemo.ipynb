{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed38b9c5-81a5-4e6b-883d-45458e4c3207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e30c02-04b1-4a96-b5c5-b1209ee6ed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Example:\n",
      "\"I would like to configure the network with the following setup:\n",
      "\n",
      "    eth1 should be up and running.\n",
      "    eth2 should be up and running.\n",
      "    bond0 should be up and running, with eth1 and eth2 configured for active-backup mode.\n",
      "    br0 should be up and running, with bond0 as its port.\n",
      "    vlan29 should be up and running, with bond0 as its base interface and VLAN ID 29.\n",
      "    br29 should be up and running, with vlan29 as its port.\"\n",
      "\n",
      "Corresponding YAML Configuration:\n",
      "interfaces:\n",
      "- name: vlan29\n",
      "  state: up\n",
      "  type: vlan\n",
      "  vlan:\n",
      "    base-iface: bond0\n",
      "    id: 29\n",
      "- bridge:\n",
      "    port:\n",
      "    - name: vlan29\n",
      "  name: br29\n",
      "  state: up\n",
      "  type: linux-bridge\n",
      "- bridge:\n",
      "    port:\n",
      "    - name: bond0\n",
      "  name: br0\n",
      "  state: up\n",
      "  type: linux-bridge\n",
      "- link-aggregation:\n",
      "    mode: active-backup\n",
      "    port:\n",
      "    - eth1\n",
      "    - eth2\n",
      "  name: bond0\n",
      "  state: up\n",
      "  type: bond\n",
      "- name: eth1\n",
      "  state: up\n",
      "  type: ethernet\n",
      "- name: eth2\n",
      "  state: up\n",
      "  type: ethernet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "def read_data(data_dir):\n",
    "    examples = []\n",
    "\n",
    "    # Iterate over files in the data directory\n",
    "    for file_name in os.listdir(data_dir):\n",
    "        if file_name.endswith('.txt'):\n",
    "            # Read text example\n",
    "            with open(os.path.join(data_dir, file_name), 'r') as file:\n",
    "                text = file.read().strip()\n",
    "\n",
    "            # Corresponding YAML file\n",
    "            yaml_file = file_name.replace('.txt', '.yml')\n",
    "            yaml_path = os.path.join(data_dir, yaml_file)\n",
    "\n",
    "            # Read YAML configuration\n",
    "            with open(yaml_path, 'r') as file:\n",
    "                yaml_config = yaml.safe_load(file)\n",
    "\n",
    "            # Append to examples list\n",
    "            examples.append({'text': text, 'yaml_config': yaml_config})\n",
    "\n",
    "    return examples\n",
    "\n",
    "# Path to the dataset directory\n",
    "data_dir = '../data'\n",
    "\n",
    "# Read data from the dataset directory\n",
    "examples = read_data(data_dir)\n",
    "\n",
    "# Display the first example\n",
    "print(\"Text Example:\")\n",
    "print(examples[0]['text'])\n",
    "print(\"\\nCorresponding YAML Configuration:\")\n",
    "print(yaml.dump(examples[0]['yaml_config']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06027ac-79e9-49b1-a9b9-06e4c996d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained GPT tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f079d6a7-bd44-4f8f-9e20-3b47452706f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a padding token in the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get the padding token ID\n",
    "padding_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# If padding token ID is None, set it to a default value (e.g., 0)\n",
    "if padding_token_id is None:\n",
    "    padding_token_id = -100  # You can adjust this value as needed\n",
    "\n",
    "\n",
    "# Define the maximum sequence length\n",
    "max_seq_length = 128\n",
    "\n",
    "# Define a custom collate function for padding\n",
    "def custom_collate(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    # Pad the sequences to the maximum length\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=float(padding_token_id))\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=float(padding_token_id))\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'labels': labels_padded\n",
    "    }\n",
    "\n",
    "# Tokenize and encode the input text\n",
    "tokenized_text = [tokenizer.encode(example['text'], return_tensors='pt', max_length=max_seq_length, padding='max_length', truncation=True) for example in examples]\n",
    "\n",
    "# Encode the YAML configurations\n",
    "encoded_yaml_configs = [tokenizer.encode(yaml.dump(example['yaml_config']), return_tensors='pt', max_length=max_seq_length, padding='max_length', truncation=True) for example in examples]\n",
    "\n",
    "\n",
    "labels = encoded_yaml_configs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d760cc7-2269-4442-8b42-ecadd6f226a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to: preprocessed_data.pt\n"
     ]
    }
   ],
   "source": [
    "# Define the file path to save the preprocessed data\n",
    "preprocessed_data_file = 'preprocessed_data.pt'\n",
    "\n",
    "# Save the preprocessed data\n",
    "torch.save({\n",
    "    'tokenized_text': tokenized_text,\n",
    "    'encoded_yaml_configs': encoded_yaml_configs,\n",
    "    'labels': labels\n",
    "}, preprocessed_data_file)\n",
    "\n",
    "print(\"Preprocessed data saved to:\", preprocessed_data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b693841-5e6c-49c6-84c8-35088a304497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thembo/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed data\n",
    "preprocessed_data = torch.load(preprocessed_data_file)\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Extract tokenized text, encoded YAML configurations, and labels from the preprocessed data\n",
    "tokenized_text = preprocessed_data['tokenized_text']\n",
    "encoded_yaml_configs = preprocessed_data['encoded_yaml_configs']\n",
    "labels = preprocessed_data['labels']\n",
    "\n",
    "# Define the GPT-2 tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Instantiate the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Define a custom dataset class to load the preprocessed data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, tokenized_text, encoded_yaml_configs, labels):\n",
    "        self.tokenized_text = tokenized_text\n",
    "        self.encoded_yaml_configs = encoded_yaml_configs\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.tokenized_text[idx],\n",
    "            'labels': self.encoded_yaml_configs[idx]\n",
    "        }\n",
    "\n",
    "# Instantiate the custom dataset\n",
    "dataset = CustomDataset(tokenized_text, encoded_yaml_configs, labels)\n",
    "\n",
    "# Define data loader for training\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define data loader for training with the custom collate function\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "num_epochs = 3\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Fine-tune the GPT-2 model\n",
    "num_epochs = 3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "fine_tuned_model_file = 'fine_tuned_gpt2_model.pt'\n",
    "torch.save(model.state_dict(), fine_tuned_model_file)\n",
    "\n",
    "print(\"Fine-tuned GPT-2 model saved to:\", fine_tuned_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f1bb8e-f546-4e4d-b8d4-2d7c8d57dc66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
